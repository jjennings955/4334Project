{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.sparse\n",
      "\n",
      "\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
      "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "tok = RegexpTokenizer(\"[a-zA-Z]+\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read in the labeled training data\n",
      "d2 = pd.read_csv('type2.csv - labeled.csv', delimiter=',')\n",
      "\n",
      "# Make sure the column names are properly assigned\n",
      "d2.columns = ['arg1', 'rel', 'arg2', 'label']\n",
      "\n",
      "# Fill in any unfilled label values as false\n",
      "d2['label'].fillna(0.0,inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Generate a binary feature vector for each column of our data. We do not apply stemming, or stopword removal. We do convert to lowercase.\n",
      "\n",
      "v1 = CountVectorizer(binary=True, tokenizer=lambda x: tok.tokenize(x), stop_words=None)\n",
      "v2 = CountVectorizer(binary=True, tokenizer=lambda x: tok.tokenize(x), stop_words=None)\n",
      "v3 = CountVectorizer(binary=True, tokenizer=lambda x: tok.tokenize(x), stop_words=None)\n",
      "\n",
      "\n",
      "arg2_features = v1.fit_transform(d2['arg2'].values)\n",
      "rel_features = v2.fit_transform(d2['rel'].values)\n",
      "arg1_features = v3.fit_transform(d2['arg1'].values)\n",
      "\n",
      "\n",
      "X = scipy.sparse.hstack((rel_features, arg1_features, arg2_features))\n",
      "#X = rel_features\n",
      "\n",
      "X = X.tocsr()\n",
      "y = d2['label'].values\n",
      "\n",
      "#data = pd.DataFrame(rows).fillna(0).as_matrix()\n",
      "#np.random.shuffle(data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SPLIT_POINT = 5000\n",
      "\n",
      "X_train = X[0:SPLIT_POINT]\n",
      "y_train = y[0:SPLIT_POINT]\n",
      "\n",
      "X_test = X[SPLIT_POINT:]\n",
      "y_test = y[SPLIT_POINT:]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.svm import LinearSVC, SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model1 = BernoulliNB()\n",
      "model2 = SVC(C=1000.0, cache_size=200, class_weight='auto', coef0=0.0, degree=3,\n",
      "  gamma=0.0001, kernel='rbf', max_iter=-1, probability=False,\n",
      "  random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "model3 = LogisticRegression()\n",
      "model4 = RandomForestClassifier(max_depth=12)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model1.fit(X_train, y_train)\n",
      "predictions1 = model1.predict(X_test)\n",
      "\n",
      "model2.fit(X_train, y_train)\n",
      "predictions2 = model2.predict(X_test)\n",
      "\n",
      "model3.fit(X_train, y_train)\n",
      "predictions3 = model3.predict(X_test)\n",
      "\n",
      "model4.fit(X_train, y_train)\n",
      "predictions4 = model4.predict(X_test)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Naive Bayes\")\n",
      "print(accuracy_score(y[SPLIT_POINT:], predictions1))\n",
      "print(precision_score(y[SPLIT_POINT:], predictions1))\n",
      "print(recall_score(y[SPLIT_POINT:], predictions1))\n",
      "print(f1_score(y[SPLIT_POINT:], predictions1))\n",
      "\n",
      "print(\"SVM - RBF\")\n",
      "print(accuracy_score(y[SPLIT_POINT:], predictions2))\n",
      "print(precision_score(y[SPLIT_POINT:], predictions2))\n",
      "print(recall_score(y[SPLIT_POINT:], predictions2))\n",
      "print(f1_score(y[SPLIT_POINT:], predictions2))\n",
      "\n",
      "print(\"Logistic Regression\")\n",
      "print(accuracy_score(y[SPLIT_POINT:], predictions3))\n",
      "print(precision_score(y[SPLIT_POINT:], predictions3))\n",
      "print(recall_score(y[SPLIT_POINT:], predictions3))\n",
      "print(f1_score(y[SPLIT_POINT:], predictions3))\n",
      "\n",
      "print(\"Decision Tree\")\n",
      "print(accuracy_score(y[SPLIT_POINT:], predictions4))\n",
      "print(precision_score(y[SPLIT_POINT:], predictions4))\n",
      "print(recall_score(y[SPLIT_POINT:], predictions4))\n",
      "print(f1_score(y[SPLIT_POINT:], predictions4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Naive Bayes\n",
        "0.892621475705\n",
        "0.847991313789\n",
        "0.95243902439\n",
        "0.89718552556\n",
        "SVM - RBF\n",
        "0.896220755849\n",
        "0.89889025894\n",
        "0.889024390244\n",
        "0.893930104231\n",
        "Logistic Regression\n",
        "0.894421115777\n",
        "0.875291375291\n",
        "0.915853658537\n",
        "0.895113230036\n",
        "Decision Tree\n",
        "0.718656268746\n",
        "0.647108130763\n",
        "0.941463414634\n",
        "0.767014406359\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_val_score(model2, X, y, cv=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 99,
       "text": [
        "array([ 0.90868263,  0.82934132,  0.92803598,  0.90404798,  0.89355322,\n",
        "        0.89189189,  0.92042042,  0.88738739,  0.88588589,  0.89039039])"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.89 (+/- 0.05)\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_val_score(model3, X, y, cv=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.89 (+/- 0.04)\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Fitting the classifier to the training set\")\n",
      "t0 = time()\n",
      "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
      "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], \n",
      "              'kernel' : ['poly', 'rbf'],\n",
      "              'degree' : [2,3,4]}\n",
      "clf = GridSearchCV(SVC(kernel='rbf', class_weight='auto'), param_grid)\n",
      "clf = clf.fit(X_train, y_train)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "print(\"Best estimator found by grid search:\")\n",
      "print(clf.best_estimator_)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting the classifier to the training set\n",
        "done in 133.239s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best estimator found by grid search:\n",
        "SVC(C=1000.0, cache_size=200, class_weight='auto', coef0=0.0, degree=2,\n",
        "  gamma=0.0001, kernel='rbf', max_iter=-1, probability=False,\n",
        "  random_state=None, shrinking=True, tol=0.001, verbose=False)\n"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictions3 = model3.predict_proba(X_test)\n",
      "pd.DataFrame(predictions3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>0.053334</td>\n",
        "      <td>0.946666</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>0.843925</td>\n",
        "      <td>0.156075</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>0.404275</td>\n",
        "      <td>0.595725</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>0.156196</td>\n",
        "      <td>0.843804</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>0.457862</td>\n",
        "      <td>0.542138</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>0.766220</td>\n",
        "      <td>0.233780</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>0.618700</td>\n",
        "      <td>0.381300</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>0.911371</td>\n",
        "      <td>0.088629</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>0.088021</td>\n",
        "      <td>0.911979</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>0.390627</td>\n",
        "      <td>0.609373</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>0.222124</td>\n",
        "      <td>0.777876</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>0.175101</td>\n",
        "      <td>0.824899</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>0.307713</td>\n",
        "      <td>0.692287</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>0.590632</td>\n",
        "      <td>0.409368</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>0.892521</td>\n",
        "      <td>0.107479</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>0.396964</td>\n",
        "      <td>0.603036</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td>0.983429</td>\n",
        "      <td>0.016571</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td>0.973795</td>\n",
        "      <td>0.026205</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td>0.994351</td>\n",
        "      <td>0.005649</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td>0.733295</td>\n",
        "      <td>0.266705</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td>0.821513</td>\n",
        "      <td>0.178487</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td>0.070479</td>\n",
        "      <td>0.929521</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td>0.103294</td>\n",
        "      <td>0.896706</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td>0.149195</td>\n",
        "      <td>0.850805</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td>0.869363</td>\n",
        "      <td>0.130637</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td>0.974433</td>\n",
        "      <td>0.025567</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td>0.682886</td>\n",
        "      <td>0.317114</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td>0.360171</td>\n",
        "      <td>0.639829</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td>0.945836</td>\n",
        "      <td>0.054164</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td>0.362670</td>\n",
        "      <td>0.637330</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1637</th>\n",
        "      <td>0.992662</td>\n",
        "      <td>0.007338</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1638</th>\n",
        "      <td>0.199742</td>\n",
        "      <td>0.800258</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1639</th>\n",
        "      <td>0.169390</td>\n",
        "      <td>0.830610</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1640</th>\n",
        "      <td>0.667657</td>\n",
        "      <td>0.332343</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1641</th>\n",
        "      <td>0.105123</td>\n",
        "      <td>0.894877</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1642</th>\n",
        "      <td>0.515914</td>\n",
        "      <td>0.484086</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1643</th>\n",
        "      <td>0.037581</td>\n",
        "      <td>0.962419</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1644</th>\n",
        "      <td>0.073663</td>\n",
        "      <td>0.926337</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1645</th>\n",
        "      <td>0.681120</td>\n",
        "      <td>0.318880</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1646</th>\n",
        "      <td>0.072036</td>\n",
        "      <td>0.927964</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1647</th>\n",
        "      <td>0.599804</td>\n",
        "      <td>0.400196</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1648</th>\n",
        "      <td>0.148817</td>\n",
        "      <td>0.851183</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1649</th>\n",
        "      <td>0.232735</td>\n",
        "      <td>0.767265</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1650</th>\n",
        "      <td>0.891499</td>\n",
        "      <td>0.108501</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1651</th>\n",
        "      <td>0.596510</td>\n",
        "      <td>0.403490</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1652</th>\n",
        "      <td>0.411550</td>\n",
        "      <td>0.588450</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1653</th>\n",
        "      <td>0.521233</td>\n",
        "      <td>0.478767</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1654</th>\n",
        "      <td>0.821030</td>\n",
        "      <td>0.178970</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1655</th>\n",
        "      <td>0.401975</td>\n",
        "      <td>0.598025</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1656</th>\n",
        "      <td>0.991105</td>\n",
        "      <td>0.008895</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1657</th>\n",
        "      <td>0.448752</td>\n",
        "      <td>0.551248</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1658</th>\n",
        "      <td>0.005820</td>\n",
        "      <td>0.994180</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1659</th>\n",
        "      <td>0.314501</td>\n",
        "      <td>0.685499</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1660</th>\n",
        "      <td>0.374782</td>\n",
        "      <td>0.625218</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1661</th>\n",
        "      <td>0.910963</td>\n",
        "      <td>0.089037</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1662</th>\n",
        "      <td>0.120998</td>\n",
        "      <td>0.879002</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1663</th>\n",
        "      <td>0.000473</td>\n",
        "      <td>0.999527</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1664</th>\n",
        "      <td>0.901050</td>\n",
        "      <td>0.098950</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1665</th>\n",
        "      <td>0.990907</td>\n",
        "      <td>0.009093</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1666</th>\n",
        "      <td>0.577421</td>\n",
        "      <td>0.422579</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>1667 rows \u00d7 2 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "             0         1\n",
        "0     0.053334  0.946666\n",
        "1     0.843925  0.156075\n",
        "2     0.404275  0.595725\n",
        "3     0.156196  0.843804\n",
        "4     0.457862  0.542138\n",
        "5     0.766220  0.233780\n",
        "6     0.618700  0.381300\n",
        "7     0.911371  0.088629\n",
        "8     0.088021  0.911979\n",
        "9     0.390627  0.609373\n",
        "10    0.222124  0.777876\n",
        "11    0.175101  0.824899\n",
        "12    0.307713  0.692287\n",
        "13    0.590632  0.409368\n",
        "14    0.892521  0.107479\n",
        "15    0.396964  0.603036\n",
        "16    0.983429  0.016571\n",
        "17    0.973795  0.026205\n",
        "18    0.994351  0.005649\n",
        "19    0.733295  0.266705\n",
        "20    0.821513  0.178487\n",
        "21    0.070479  0.929521\n",
        "22    0.103294  0.896706\n",
        "23    0.149195  0.850805\n",
        "24    0.869363  0.130637\n",
        "25    0.974433  0.025567\n",
        "26    0.682886  0.317114\n",
        "27    0.360171  0.639829\n",
        "28    0.945836  0.054164\n",
        "29    0.362670  0.637330\n",
        "...        ...       ...\n",
        "1637  0.992662  0.007338\n",
        "1638  0.199742  0.800258\n",
        "1639  0.169390  0.830610\n",
        "1640  0.667657  0.332343\n",
        "1641  0.105123  0.894877\n",
        "1642  0.515914  0.484086\n",
        "1643  0.037581  0.962419\n",
        "1644  0.073663  0.926337\n",
        "1645  0.681120  0.318880\n",
        "1646  0.072036  0.927964\n",
        "1647  0.599804  0.400196\n",
        "1648  0.148817  0.851183\n",
        "1649  0.232735  0.767265\n",
        "1650  0.891499  0.108501\n",
        "1651  0.596510  0.403490\n",
        "1652  0.411550  0.588450\n",
        "1653  0.521233  0.478767\n",
        "1654  0.821030  0.178970\n",
        "1655  0.401975  0.598025\n",
        "1656  0.991105  0.008895\n",
        "1657  0.448752  0.551248\n",
        "1658  0.005820  0.994180\n",
        "1659  0.314501  0.685499\n",
        "1660  0.374782  0.625218\n",
        "1661  0.910963  0.089037\n",
        "1662  0.120998  0.879002\n",
        "1663  0.000473  0.999527\n",
        "1664  0.901050  0.098950\n",
        "1665  0.990907  0.009093\n",
        "1666  0.577421  0.422579\n",
        "\n",
        "[1667 rows x 2 columns]"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v1.inverse_transform(arg2_features[13])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "[array([u'the', u'salad', u'arcadia'], \n",
        "      dtype='<U15')]"
       ]
      }
     ],
     "prompt_number": 50
    }
   ],
   "metadata": {}
  }
 ]
}